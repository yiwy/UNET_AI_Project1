import torch  # torch provides basic functions, to creating tensors.
import torch.nn as nn  # torch.nn allows us to create a neural network. This module torch.nn
# also has various layers that you can use to build your neural network. For example, nn.Linear
import torch.nn.functional as F  # nn.functional give us access to the activation and loss functions.
from torch.optim import Adam  # optim contains many optimizers. Here Adam is used.

import lightning as L  # this makes neural networks easier to train
from torch.utils.data import TensorDataset, DataLoader  # these are needed for the training data
# case working with large datasets.import

from pytorch_lightning.utilities.seed import seed_everything

inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])
labels = torch.tensor([0., 1.])
dataset = TensorDataset(inputs, labels)
dataloader = DataLoader(dataset)

# Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()


class LightningLSTM(L.LightningModule):

    def __init__(self):   # __init__() is the class constructor function,
                         #  and we use it to initialize the Weights and Biases.

        super().__init__()  # initialize an instance of the parent class, LightningModule.

        seed_everything(seed=42)

        # input_size = number of features (or variables) in the data. In our example
        #              we only have a single feature (value)
        # hidden_size = this determines the dimension of the output
        #               in other words, if we set hidden_size=1, then we have 1 output node
        #               if we set hidden_size=50, then we hve 50 output nodes (that can then be 50 input
        #               nodes to a subsequent fully connected neural network.
        self.lstm = nn.LSTM(input_size=1, hidden_size=1)

    def forward(self, input):
        # transpose the input vector
        input_trans = input.view(len(input), 1)

        lstm_out, temp = self.lstm(input_trans)

        # lstm_out has the short-term memories for all inputs. We make our prediction with the last one
        prediction = lstm_out[-1]
        return prediction

    def configure_optimizers(self):  # this configures the optimizer we want to use for backpropagation.
        return Adam(self.parameters(), lr=0.1)  # we'll just go ahead and set the learning rate to 0.1

    def training_step(self, batch, batch_idx):  # take a step during gradient descent.
        input_i, label_i = batch  # collect input
        output_i = self.forward(input_i[0])  # run input through the neural network
        loss = (output_i - label_i) ** 2  # loss = squared residual

        ###################
        ##
        ## Logging the loss and the predicted values so we can evaluate the training
        ##
        ###################
        self.log("train_loss", loss)

        if (label_i == 0):
            self.log("out_0", output_i)
        else:
            self.log("out_1", output_i)

        return loss


# # Creating a new model, and printing the values.
#
# model = LightningLSTM()  # First, make model from the class
#
# # print out the name and value for each parameter
# print("Before optimization, the parameters are...")
# for name, param in model.named_parameters():
#     print(name, param.data)
#
# print("\nComparing the observed and predicted values...")
# print("Company A: Observed = 0, Predicted =", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())
# print("Company B: Observed = 1, Predicted =", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())
#
# # on this first attempt, the predicted model are bad. Model needs training
# # However, because we've increased the learning rate to 0.1, we only need to train for 300 epochs.
#
# # NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.
# # Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train
# # the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,
# # we have to tell the trainer add stuff to the log files every 2 steps (or epoch,
# # since we have to rows of training data)
# # because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So
# trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)
#
# trainer.fit(model, train_dataloaders=dataloader)
#
# print("After optimization, the parameters are...")
# for name, param in model.named_parameters():
#     print(name, param.data)
# # competing training, the predictions are printed.
#
# print("\nNow let's compare the observed and predicted values...")
# print("Company A: Observed = 0, Predicted =", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())
# print("Company B: Observed = 1, Predicted =", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())
#
# # After just 300 epochs, the LSTM is making great predictions.
# # the prediction for Company A is close to the observed value 0 and
# # the prediction for Company B is close to the observed value 1
